{"cells":[{"cell_type":"code","source":["import dlt\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n# The following code has Decorators which can not be run on a Compute but as a Job.\n# Following 3 commands are representing improvisation of Delta Tables from Bronze to Gold levels.\n\n# Define a path to the data source files in a python variable.\njson_path = \"/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json\"\n# The 'dlt' is a decorater of Python.\n# The dlt.table is like a create table function. \n# It may define comment, table name and so on. \n# It must be followed by a function which returns a data frame from which the dlt.table will create a table. \n# The name of the delta table will be name of a function 'clickstream_raw' if not defined explicitly.\n# Note this delta table is representing a Bronze Table\n@dlt.table(   # The @dlt.create_table() also works.\n  comment=\"The raw wikipedia clickstream dataset, ingested from /databricks-datasets.\"\n)\ndef clickstream_raw():\n  return (spark.read.format(\"json\").load(json_path))\n\n\n# Create a new Silver delta table from a Bronze delta table defined in earlier step. \n# Find here, two new decoraters- @dlt.expect() and @dlt.expect_or_fail to define the expectation. Multiple @dlt.expect decoraters are applicable.\n# The expect decorators are applied while data enters into a target data frame and they are to measure the data quality.\n# Also observe, dlt.read() to read data from a Delta Table (spark.read() reads a data from a file).\n# The dlt.read() is also creating a dependency to show that clickstream_raw() must be executed first to run clickstream_prepared().\n# Note this delta table is representing a Silver Table.\n@dlt.table(\n  comment=\"Wikipedia clickstream data cleaned and prepared for analysis.\"\n)\n@dlt.expect(\"valid_current_page_title\", \"current_page_title IS NOT NULL\")\n@dlt.expect_or_fail(\"valid_count\", \"click_count > 0\")\ndef clickstream_prepared():\n  return (\n    dlt.read(\"clickstream_raw\")\n      .withColumn(\"click_count\", expr(\"CAST(n AS INT)\"))\n      .withColumnRenamed(\"curr_title\", \"current_page_title\")\n      .withColumnRenamed(\"prev_title\", \"previous_page_title\")\n      .select(\"current_page_title\", \"click_count\", \"previous_page_title\")\n  )\n\n# Create a new Gold delta table from a Silver Delta table defined in earlier step.\n# The dlt.read() is defining a dependency on Silver Table\n@dlt.table(\n  comment=\"A table containing the top pages linking to the Apache Spark page.\"\n)\ndef top_spark_referrers():\n  return (\n    dlt.read(\"clickstream_prepared\")\n      .filter(expr(\"current_page_title == 'Apache_Spark'\"))\n      .withColumnRenamed(\"previous_page_title\", \"referrer\")\n      .sort(desc(\"click_count\"))\n      .select(\"referrer\", \"click_count\")\n      .limit(10)\n  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cdc9cd74-59cb-4a78-bb18-f2d7fc46d98f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ModuleNotFoundError</span>                       Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1417054828990772&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">import</span> dlt\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>functions <span class=\"ansi-green-fg\">import</span> <span class=\"ansi-blue-fg\">*</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>types <span class=\"ansi-green-fg\">import</span> <span class=\"ansi-blue-fg\">*</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> json_path <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json&#34;</span>\n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py</span> in <span class=\"ansi-cyan-fg\">import_patch</span><span class=\"ansi-blue-fg\">(name, globals, locals, fromlist, level)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    165</span>             <span class=\"ansi-red-fg\"># Import the desired module. If you’re seeing this while debugging a failed import,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    166</span>             <span class=\"ansi-red-fg\"># look at preceding stack frames for relevant error information.</span>\n<span class=\"ansi-green-fg\">--&gt; 167</span><span class=\"ansi-red-fg\">             </span>original_result <span class=\"ansi-blue-fg\">=</span> python_builtin_import<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">,</span> globals<span class=\"ansi-blue-fg\">,</span> locals<span class=\"ansi-blue-fg\">,</span> fromlist<span class=\"ansi-blue-fg\">,</span> level<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    168</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    169</span>             is_root_import <span class=\"ansi-blue-fg\">=</span> thread_local<span class=\"ansi-blue-fg\">.</span>_nest_level <span class=\"ansi-blue-fg\">==</span> <span class=\"ansi-cyan-fg\">1</span>\n\n<span class=\"ansi-red-fg\">ModuleNotFoundError</span>: No module named &#39;dlt&#39;</div>","errorSummary":"<span class=\"ansi-red-fg\">ModuleNotFoundError</span>: No module named &#39;dlt&#39;","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ModuleNotFoundError</span>                       Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1417054828990772&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">import</span> dlt\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>functions <span class=\"ansi-green-fg\">import</span> <span class=\"ansi-blue-fg\">*</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>types <span class=\"ansi-green-fg\">import</span> <span class=\"ansi-blue-fg\">*</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> json_path <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json&#34;</span>\n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py</span> in <span class=\"ansi-cyan-fg\">import_patch</span><span class=\"ansi-blue-fg\">(name, globals, locals, fromlist, level)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    165</span>             <span class=\"ansi-red-fg\"># Import the desired module. If you’re seeing this while debugging a failed import,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    166</span>             <span class=\"ansi-red-fg\"># look at preceding stack frames for relevant error information.</span>\n<span class=\"ansi-green-fg\">--&gt; 167</span><span class=\"ansi-red-fg\">             </span>original_result <span class=\"ansi-blue-fg\">=</span> python_builtin_import<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">,</span> globals<span class=\"ansi-blue-fg\">,</span> locals<span class=\"ansi-blue-fg\">,</span> fromlist<span class=\"ansi-blue-fg\">,</span> level<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    168</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    169</span>             is_root_import <span class=\"ansi-blue-fg\">=</span> thread_local<span class=\"ansi-blue-fg\">.</span>_nest_level <span class=\"ansi-blue-fg\">==</span> <span class=\"ansi-cyan-fg\">1</span>\n\n<span class=\"ansi-red-fg\">ModuleNotFoundError</span>: No module named &#39;dlt&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9ba2158-f3bb-4be6-8352-d5437791b909"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["This Delta Live Tables query is syntactically valid, but you must create a pipeline in order to define and populate your table."]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"message","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>message</th></tr></thead><tbody><tr><td>This Delta Live Tables query is syntactically valid, but you must create a pipeline in order to define and populate your table.</td></tr></tbody></table></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Notebook210_DeltaLiveTablesWithPySpark","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1417054828990771}},"nbformat":4,"nbformat_minor":0}
